{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda env : kyuwon_video_swin_transformer(Python 3.10.11)\n",
    "---\n",
    "#### [주의사항]\n",
    "##### Image Size가 (224,224,3)인 경우 Model Parameter를 아래와 같이 고정할 것\n",
    "##### model = SwinTRansformer3D(patch_size=(4,4,4), embed_dim=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# 사용하고자 하는 model\n",
    "# Fat_Transformer\n",
    "# from Fat_Transformer.Only_Fat_transformer.model import SwinTransformer3D\n",
    "\n",
    "# R(2+1)D + Fat_Transformer\n",
    "from Fat_Transformer.R2plus1d_Fat_Transformer.model import SwinTransformer3D\n",
    "\n",
    "# 2D_Patch_Partition, R(2+1)D + Fat_Transformer\n",
    "# from Video_Swin_Transformer.Patch_Partition_R2plus1d_Video_Swin_Transformer.model import SwinTransformer3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 8\n",
    "num_workers = 8\n",
    "epochs = 120\n",
    "start_epoch = 0\n",
    "lr = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "device_1 = torch.device('cuda:1' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/training_set.dat', \"rb\") as training_file:\n",
    "    train_input_temp = pickle.load(training_file)\n",
    "with open('/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/validation_set.dat', \"rb\") as training_file:\n",
    "    valid_input_temp = pickle.load(training_file)\n",
    "    \n",
    "# 15 Frame / Full-Frame / 224x224x3:/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/training_set.dat , /home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/validation_set.dat\n",
    "# 15 Frame / Face / 224x224x3 : \n",
    "# 15 Frame / Face / 128x128x3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChalearnDataset(Dataset):\n",
    "    def __init__(self,imagedata,tagdata,transform=None):\n",
    "        self.imagedata=imagedata\n",
    "        self.tagdata=tagdata\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imagedata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_data=self.imagedata[idx]\n",
    "        image_data=torch.FloatTensor(image_data)\n",
    "        big_five_sorces=self.tagdata[idx]\n",
    "        big_five_sorces = torch.FloatTensor(big_five_sorces)\n",
    "        return image_data,big_five_sorces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChalearnDataset(imagedata=train_input_temp[0],tagdata=train_input_temp[1])\n",
    "val_dataset = ChalearnDataset(imagedata=valid_input_temp[0],tagdata=valid_input_temp[1])\n",
    "# test_dataset = ChalearnDataset(imagedata=test_set_data,tagdata=test_y,transform=transform)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workers)\n",
    "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)\n",
    "max_value=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2plus1d_pretrained_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model=SwinTransformer3D(patch_size=(4,4,4), embed_dim=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, input_size = (4,3,15,224,224), col_names = ['input_size','output_size','num_params'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint=torch.load(\"saved_model/model_0.pth\", map_location=device)\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "# start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = nn.DataParallel(model, device_ids = [0,1])\n",
    "\n",
    "criterion = torch.nn.L1Loss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/748 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/kw/개성형성/ISSR_CBNU/Fat_Transformer/R2plus1d_Fat_Transformer/model.py\", line 846, in forward\n    x = self.r2plus1d_backbone(x) # 2D Patch Partition + R(2+1)D Backbone + Reshape\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/kw/개성형성/ISSR_CBNU/Fat_Transformer/R2plus1d_Fat_Transformer/custom_resnet.py\", line 232, in forward\n    x = self.layer1(x)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/kw/개성형성/ISSR_CBNU/Fat_Transformer/R2plus1d_Fat_Transformer/custom_resnet.py\", line 108, in forward\n    out = self.conv1(x)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\nRuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ssrlab/kw/개성형성/ISSR_CBNU/multi_gpu_test.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/multi_gpu_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m big_five_data \u001b[39m=\u001b[39m big_five_data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/multi_gpu_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/multi_gpu_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m model(image_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/multi_gpu_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(hypothesis, big_five_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/multi_gpu_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/kw/개성형성/ISSR_CBNU/Fat_Transformer/R2plus1d_Fat_Transformer/model.py\", line 846, in forward\n    x = self.r2plus1d_backbone(x) # 2D Patch Partition + R(2+1)D Backbone + Reshape\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/kw/개성형성/ISSR_CBNU/Fat_Transformer/R2plus1d_Fat_Transformer/custom_resnet.py\", line 232, in forward\n    x = self.layer1(x)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/kw/개성형성/ISSR_CBNU/Fat_Transformer/R2plus1d_Fat_Transformer/custom_resnet.py\", line 108, in forward\n    out = self.conv1(x)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n  File \"/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\nRuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR\n"
     ]
    }
   ],
   "source": [
    "trainingEpoch_loss = []\n",
    "validationEpoch_loss = []\n",
    "\n",
    "for i in range(120):\n",
    "    train_avg_loss = 0\n",
    "    val_avg_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for image_data, big_five_data in tqdm(train_dataloader):\n",
    "        \n",
    "        image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "        image_data = image_data.to(device_1)\n",
    "        \n",
    "        big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "        big_five_data = big_five_data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        hypothesis = model(image_data)\n",
    "        \n",
    "        loss = criterion(hypothesis, big_five_data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_avg_loss += loss\n",
    "    train_avg_loss=train_avg_loss/len(train_dataloader)\n",
    "    trainingEpoch_loss.append(train_avg_loss)\n",
    "    print('Epoch = {}, 1 - train_loss = {}'.format(i+1,(1 - train_avg_loss)))\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for image_data, big_five_data in tqdm(val_dataloader):\n",
    "            \n",
    "            image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "            image_data = image_data.to(device_1)\n",
    "            \n",
    "            big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "            big_five_data = big_five_data.to(device)\n",
    "            \n",
    "            hypothesis = model(image_data)\n",
    "            \n",
    "            val_loss = criterion(hypothesis, big_five_data)\n",
    "            val_avg_loss += val_loss\n",
    "            \n",
    "        val_avg_loss=val_avg_loss/len(val_dataloader)\n",
    "        validationEpoch_loss.append(val_avg_loss)\n",
    "    print('Epoch = {}, 1 - val_loss = {}'.format(i+1,(1 - val_avg_loss)))\n",
    "    print('\\n')\n",
    "    \n",
    "    start_epoch+=1\n",
    "\n",
    "    # torch.save({\n",
    "    #     'epoch': i+1,\n",
    "    #     'model': model.state_dict(),\n",
    "    #     'optimizer': optimizer.state_dict(),\n",
    "    #     'loss': val_avg_loss,\n",
    "    # }, save_model_file_path.format('model',start_epoch,'pth'))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "# with torch.cuda.device(1):\n",
    "    \n",
    "#     trainingEpoch_loss = []\n",
    "#     validationEpoch_loss = []\n",
    "    \n",
    "#     for i in range(120):\n",
    "#         train_avg_loss = 0\n",
    "#         val_avg_loss = 0\n",
    "        \n",
    "#         model.train()\n",
    "#         for image_data, big_five_data in tqdm(train_dataloader):\n",
    "            \n",
    "#             image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "#             image_data = image_data.to(device)\n",
    "            \n",
    "#             big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "#             big_five_data = big_five_data.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             hypothesis = model(image_data)\n",
    "            \n",
    "#             loss = criterion(hypothesis, big_five_data)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_avg_loss += loss\n",
    "#         train_avg_loss=train_avg_loss/len(train_dataloader)\n",
    "#         trainingEpoch_loss.append(train_avg_loss)\n",
    "#         print('Epoch = {}, 1 - train_loss = {}'.format(i+1,(1 - train_avg_loss)))\n",
    "        \n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             model.eval()\n",
    "#             for image_data, big_five_data in tqdm(val_dataloader):\n",
    "                \n",
    "#                 image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "#                 image_data = image_data.to(device)\n",
    "                \n",
    "#                 big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "#                 big_five_data = big_five_data.to(device)\n",
    "                \n",
    "#                 hypothesis = model(image_data)\n",
    "                \n",
    "#                 val_loss = criterion(hypothesis, big_five_data)\n",
    "#                 val_avg_loss += val_loss\n",
    "                \n",
    "#             val_avg_loss=val_avg_loss/len(val_dataloader)\n",
    "#             validationEpoch_loss.append(val_avg_loss)\n",
    "#         print('Epoch = {}, 1 - val_loss = {}'.format(i+1,(1 - val_avg_loss)))\n",
    "#         print('\\n')\n",
    "        \n",
    "#         start_epoch+=1\n",
    "    \n",
    "#         # torch.save({\n",
    "#         #     'epoch': i+1,\n",
    "#         #     'model': model.state_dict(),\n",
    "#         #     'optimizer': optimizer.state_dict(),\n",
    "#         #     'loss': val_avg_loss,\n",
    "#         # }, save_model_file_path.format('model',start_epoch,'pth'))\n",
    "        \n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = []\n",
    "temp_2 = []\n",
    "for i in range(len(trainingEpoch_loss)):\n",
    "    temp_1.append(trainingEpoch_loss[i].item())\n",
    "    temp_2.append(validationEpoch_loss[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(temp_1, label='train_loss')\n",
    "plt.plot(temp_2, label='val_loss')\n",
    "# plt.title('')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('1 - MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(temp_2, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('1 - MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
