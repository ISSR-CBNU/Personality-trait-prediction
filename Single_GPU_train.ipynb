{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda env : kyuwon_video_swin_transformer(Python 3.10.11)\n",
    "---\n",
    "#### [주의사항]\n",
    "##### Image Size가 (224,224,3)인 경우 Model Parameter를 아래와 같이 고정할 것\n",
    "##### model = SwinTRansformer3D(patch_size=(4,4,4), embed_dim=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# 사용하고자 하는 model\n",
    "# Video_Swin_Transformer\n",
    "# from Video_Swin_Transformer.Video_Swin_Transformer.model import SwinTransformer3D\n",
    "\n",
    "# R(2+1)D + Video_Swin_Transformer\n",
    "from Video_Swin_Transformer.R2plus1d_Video_Swin_Transformer.model import SwinTransformer3D\n",
    "\n",
    "# 2D_Patch_Partition, R(2+1)D + Video_Swin_Transformer\n",
    "# from Video_Swin_Transformer.Patch_Partition_R2plus1d_Video_Swin_Transformer.model import SwinTransformer3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 8\n",
    "num_workers = 8\n",
    "epochs = 120\n",
    "start_epoch = 0\n",
    "lr = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/training_set.dat', \"rb\") as training_file:\n",
    "    train_input_temp = pickle.load(training_file)\n",
    "with open('/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/validation_set.dat', \"rb\") as training_file:\n",
    "    valid_input_temp = pickle.load(training_file)\n",
    "    \n",
    "# 15 Frame / Full-Frame / 224x224x3:/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/training_set.dat , /home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/validation_set.dat\n",
    "# 15 Frame / Face / 224x224x3 : \n",
    "# 15 Frame / Face / 128x128x3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChalearnDataset(Dataset):\n",
    "    def __init__(self,imagedata,tagdata,transform=None):\n",
    "        self.imagedata=imagedata\n",
    "        self.tagdata=tagdata\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imagedata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_data=self.imagedata[idx]\n",
    "        image_data=torch.FloatTensor(image_data)\n",
    "        big_five_sorces=self.tagdata[idx]\n",
    "        big_five_sorces = torch.FloatTensor(big_five_sorces)\n",
    "        return image_data,big_five_sorces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChalearnDataset(imagedata=train_input_temp[0],tagdata=train_input_temp[1])\n",
    "val_dataset = ChalearnDataset(imagedata=valid_input_temp[0],tagdata=valid_input_temp[1])\n",
    "# test_dataset = ChalearnDataset(imagedata=test_set_data,tagdata=test_y,transform=transform)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workers)\n",
    "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)\n",
    "max_value=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SwinTransformer3D(patch_size=(4,4,4), embed_dim=96)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.L1Loss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, input_size = (4,3,15,224,224), col_names = ['input_size','output_size','num_params'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint=torch.load(\"saved_model/model_0.pth\", map_location=device)\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "# start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    \n",
    "    trainingEpoch_loss = []\n",
    "    validationEpoch_loss = []\n",
    "    \n",
    "    for i in range(120):\n",
    "        train_avg_loss = 0\n",
    "        val_avg_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for image_data, big_five_data in tqdm(train_dataloader):\n",
    "            \n",
    "            image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "            image_data = image_data.to(device)\n",
    "            \n",
    "            big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "            big_five_data = big_five_data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            hypothesis = model(image_data)\n",
    "            \n",
    "            loss = criterion(hypothesis, big_five_data)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_avg_loss += loss\n",
    "        train_avg_loss=train_avg_loss/len(train_dataloader)\n",
    "        trainingEpoch_loss.append(train_avg_loss)\n",
    "        print('Epoch = {}, 1 - train_loss = {}'.format(i+1,(1 - train_avg_loss)))\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for image_data, big_five_data in tqdm(val_dataloader):\n",
    "                \n",
    "                image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "                image_data = image_data.to(device)\n",
    "                \n",
    "                big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "                big_five_data = big_five_data.to(device)\n",
    "                \n",
    "                hypothesis = model(image_data)\n",
    "                \n",
    "                val_loss = criterion(hypothesis, big_five_data)\n",
    "                val_avg_loss += val_loss\n",
    "                \n",
    "            val_avg_loss=val_avg_loss/len(val_dataloader)\n",
    "            validationEpoch_loss.append(val_avg_loss)\n",
    "        print('Epoch = {}, 1 - val_loss = {}'.format(i+1,(1 - val_avg_loss)))\n",
    "        print('\\n')\n",
    "        \n",
    "        start_epoch+=1\n",
    "    \n",
    "        # torch.save({\n",
    "        #     'epoch': i+1,\n",
    "        #     'model': model.state_dict(),\n",
    "        #     'optimizer': optimizer.state_dict(),\n",
    "        #     'loss': val_avg_loss,\n",
    "        # }, save_model_file_path.format('model',start_epoch,'pth'))\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = []\n",
    "temp_2 = []\n",
    "for i in range(len(trainingEpoch_loss)):\n",
    "    temp_1.append(trainingEpoch_loss[i].item())\n",
    "    temp_2.append(validationEpoch_loss[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(temp_1, label='train_loss')\n",
    "plt.plot(temp_2, label='val_loss')\n",
    "# plt.title('')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('1 - MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(temp_2, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('1 - MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
