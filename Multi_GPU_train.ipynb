{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda env : kyuwon_video_swin_transformer(Python 3.10.11)\n",
    "---\n",
    "#### [주의사항]\n",
    "##### Image Size가 (224,224,3)인 경우 Model Parameter를 아래와 같이 고정할 것\n",
    "##### model = SwinTRansformer3D(patch_size=(4,4,4), embed_dim=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-10 02:19:35.322327: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-10 02:19:35.350108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-10 02:19:35.710274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# 사용하고자 하는 model\n",
    "# Fat_Transformer\n",
    "from Fat_Transformer.Only_Fat_Transformer.model import SwinTransformer3D\n",
    "# R(2+1)D + Fat_Transformer\n",
    "# from Fat_Transformer.R2plus1d_Fat_Transformer.model import SwinTransformer3D\n",
    "\n",
    "# 2D_Patch_Partition, R(2+1)D + Fat_Transformer\n",
    "# from Video_Swin_Transformer.Patch_Partition_R2plus1d_Video_Swin_Transformer.model import SwinTransformer3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 64\n",
    "num_workers = 8\n",
    "epochs = 120\n",
    "start_epoch = 0\n",
    "lr = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/training_set.dat', \"rb\") as training_file:\n",
    "    train_input_temp = pickle.load(training_file)\n",
    "with open('/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/validation_set.dat', \"rb\") as training_file:\n",
    "    valid_input_temp = pickle.load(training_file)\n",
    "    \n",
    "# 15 Frame / Full-Frame / 224x224x3:/home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/training_set.dat , /home/ssrlab/kw/개성형성/video_swin_transformer/Data_Processing/15Frame/Full-Shot/validation_set.dat\n",
    "# 15 Frame / Face / 224x224x3 : \n",
    "# 15 Frame / Face / 128x128x3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChalearnDataset(Dataset):\n",
    "    def __init__(self,imagedata,tagdata,transform=None):\n",
    "        self.imagedata=imagedata\n",
    "        self.tagdata=tagdata\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imagedata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_data=self.imagedata[idx]\n",
    "        image_data=torch.FloatTensor(image_data)\n",
    "        big_five_sorces=self.tagdata[idx]\n",
    "        big_five_sorces = torch.FloatTensor(big_five_sorces)\n",
    "        return image_data,big_five_sorces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChalearnDataset(imagedata=train_input_temp[0],tagdata=train_input_temp[1])\n",
    "val_dataset = ChalearnDataset(imagedata=valid_input_temp[0],tagdata=valid_input_temp[1])\n",
    "# test_dataset = ChalearnDataset(imagedata=test_set_data,tagdata=test_y,transform=transform)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workers)\n",
    "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)\n",
    "max_value=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssrlab/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model=SwinTransformer3D(patch_size=(4,4,4), embed_dim=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, input_size = (4,3,15,224,224), col_names = ['input_size','output_size','num_params'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint=torch.load(\"saved_model/model_0.pth\", map_location=device)\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "# start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = nn.DataParallel(model, device_ids = [0,1])\n",
    "\n",
    "criterion = torch.nn.L1Loss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/374 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ssrlab/kw/개성형성/ISSR_CBNU/Multi_GPU_train.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/Multi_GPU_train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m big_five_data \u001b[39m=\u001b[39m big_five_data\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/Multi_GPU_train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/Multi_GPU_train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m model(image_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/Multi_GPU_train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(hypothesis, big_five_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.88/home/ssrlab/kw/%EA%B0%9C%EC%84%B1%ED%98%95%EC%84%B1/ISSR_CBNU/Multi_GPU_train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     79\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     80\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[0;32m---> 81\u001b[0m         thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m], streams[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/kyuwon_video_swin_transformer/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainingEpoch_loss = []\n",
    "validationEpoch_loss = []\n",
    "\n",
    "for i in range(120):\n",
    "    train_avg_loss = 0\n",
    "    val_avg_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for image_data, big_five_data in tqdm(train_dataloader):\n",
    "        \n",
    "        image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "        image_data = image_data.cuda()\n",
    "        \n",
    "        big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "        big_five_data = big_five_data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(image_data)\n",
    "        \n",
    "        loss = criterion(hypothesis, big_five_data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_avg_loss += loss\n",
    "    train_avg_loss=train_avg_loss/len(train_dataloader)\n",
    "    trainingEpoch_loss.append(train_avg_loss)\n",
    "    print('Epoch = {}, 1 - train_loss = {}'.format(i+1,(1 - train_avg_loss)))\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for image_data, big_five_data in tqdm(val_dataloader):\n",
    "            \n",
    "            image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "            image_data = image_data.cuda()\n",
    "            \n",
    "            big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "            big_five_data = big_five_data.cuda()\n",
    "            \n",
    "            hypothesis = model(image_data)\n",
    "            \n",
    "            val_loss = criterion(hypothesis, big_five_data)\n",
    "            val_avg_loss += val_loss\n",
    "            \n",
    "        val_avg_loss=val_avg_loss/len(val_dataloader)\n",
    "        validationEpoch_loss.append(val_avg_loss)\n",
    "    print('Epoch = {}, 1 - val_loss = {}'.format(i+1,(1 - val_avg_loss)))\n",
    "    print('\\n')\n",
    "    \n",
    "    start_epoch+=1\n",
    "\n",
    "    # torch.save({\n",
    "    #     'epoch': i+1,\n",
    "    #     'model': model.state_dict(),\n",
    "    #     'optimizer': optimizer.state_dict(),\n",
    "    #     'loss': val_avg_loss,\n",
    "    # }, save_model_file_path.format('model',start_epoch,'pth'))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "# with torch.cuda.device(1):\n",
    "    \n",
    "#     trainingEpoch_loss = []\n",
    "#     validationEpoch_loss = []\n",
    "    \n",
    "#     for i in range(120):\n",
    "#         train_avg_loss = 0\n",
    "#         val_avg_loss = 0\n",
    "        \n",
    "#         model.train()\n",
    "#         for image_data, big_five_data in tqdm(train_dataloader):\n",
    "            \n",
    "#             image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "#             image_data = image_data.to(device)\n",
    "            \n",
    "#             big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "#             big_five_data = big_five_data.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             hypothesis = model(image_data)\n",
    "            \n",
    "#             loss = criterion(hypothesis, big_five_data)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_avg_loss += loss\n",
    "#         train_avg_loss=train_avg_loss/len(train_dataloader)\n",
    "#         trainingEpoch_loss.append(train_avg_loss)\n",
    "#         print('Epoch = {}, 1 - train_loss = {}'.format(i+1,(1 - train_avg_loss)))\n",
    "        \n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             model.eval()\n",
    "#             for image_data, big_five_data in tqdm(val_dataloader):\n",
    "                \n",
    "#                 image_data = rearrange(image_data, 'b d h w c -> b c d h w')\n",
    "#                 image_data = image_data.to(device)\n",
    "                \n",
    "#                 big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "#                 big_five_data = big_five_data.to(device)\n",
    "                \n",
    "#                 hypothesis = model(image_data)\n",
    "                \n",
    "#                 val_loss = criterion(hypothesis, big_five_data)\n",
    "#                 val_avg_loss += val_loss\n",
    "                \n",
    "#             val_avg_loss=val_avg_loss/len(val_dataloader)\n",
    "#             validationEpoch_loss.append(val_avg_loss)\n",
    "#         print('Epoch = {}, 1 - val_loss = {}'.format(i+1,(1 - val_avg_loss)))\n",
    "#         print('\\n')\n",
    "        \n",
    "#         start_epoch+=1\n",
    "    \n",
    "#         # torch.save({\n",
    "#         #     'epoch': i+1,\n",
    "#         #     'model': model.state_dict(),\n",
    "#         #     'optimizer': optimizer.state_dict(),\n",
    "#         #     'loss': val_avg_loss,\n",
    "#         # }, save_model_file_path.format('model',start_epoch,'pth'))\n",
    "        \n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = []\n",
    "temp_2 = []\n",
    "for i in range(len(trainingEpoch_loss)):\n",
    "    temp_1.append(trainingEpoch_loss[i].item())\n",
    "    temp_2.append(validationEpoch_loss[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(temp_1, label='train_loss')\n",
    "plt.plot(temp_2, label='val_loss')\n",
    "# plt.title('')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('1 - MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(temp_2, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('1 - MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw_sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
