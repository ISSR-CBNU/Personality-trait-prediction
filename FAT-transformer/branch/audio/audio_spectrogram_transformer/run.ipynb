{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from facenet_pytorch import MTCNN\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ASTModel\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Create a new class that inherits the original ASTModel class\n",
    "class ASTModelVis(ASTModel):\n",
    "    def get_att_map(self, block, x):\n",
    "        qkv = block.attn.qkv\n",
    "        num_heads = block.attn.num_heads\n",
    "        scale = block.attn.scale\n",
    "        B, N, C = x.shape\n",
    "        qkv = qkv(x).reshape(B, N, 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "        attn = (q @ k.transpose(-2, -1)) * scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        return attn\n",
    "\n",
    "    def forward_visualization(self, x):\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        B = x.shape[0]\n",
    "        x = self.v.patch_embed(x)\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "        # save the attention map of each of 12 Transformer layer\n",
    "        att_list = []\n",
    "        for blk in self.v.blocks:\n",
    "            cur_att = self.get_att_map(blk, x)\n",
    "            att_list.append(cur_att)\n",
    "            x = blk(x)\n",
    "        return att_list\n",
    "\n",
    "def make_features(wav_name, mel_bins, target_length=1024):\n",
    "    waveform, sr = torchaudio.load(wav_name)\n",
    "    assert sr == 16000, 'input audio sampling rate must be 16kHz'\n",
    "\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(\n",
    "        waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
    "        window_type='hanning', num_mel_bins=mel_bins, dither=0.0, frame_shift=10)\n",
    "\n",
    "    n_frames = fbank.shape[0]\n",
    "\n",
    "    p = target_length - n_frames\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:target_length, :]\n",
    "\n",
    "    fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)\n",
    "    return fbank\n",
    "\n",
    "\n",
    "def load_label(label_csv):\n",
    "    with open(label_csv, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        lines = list(reader)\n",
    "    labels = []\n",
    "    ids = []  # Each label has a unique id such as \"/m/068hy\"\n",
    "    for i1 in range(1, len(lines)):\n",
    "        id = lines[i1][1]\n",
    "        label = lines[i1][2]\n",
    "        ids.append(id)\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "# Assume each input spectrogram has 1024 time frames\n",
    "input_tdim = 1054\n",
    "# now load the visualization model\n",
    "audio_model = ASTModelVis(label_dim=5, input_tdim=input_tdim, imagenet_pretrain=False, audioset_pretrain=False)\n",
    "\n",
    "audio_model = audio_model.to(torch.device(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ssrlab/kw/개성형성/audio_spectrogram_transformer/training_set_audio_spectrogram', \"rb\") as training_file:\n",
    "    train_set_data = pickle.load(training_file)\n",
    "with open('/home/ssrlab/kw/개성형성/audio_spectrogram_transformer/validation_set_audio_spectrogram', \"rb\") as training_file:\n",
    "    valid_set_data = pickle.load(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_expected_input(dataset: List[Tuple[np.ndarray,np.ndarray]]) -> Tuple[np.ndarray,np.ndarray]:\n",
    "    \n",
    "    x0_list = []\n",
    "    x1_list = []\n",
    "    y_list = []\n",
    "    for i in range(0,len(dataset)):\n",
    "        x0_list.append(dataset[i][0])\n",
    "        x1_list.append(dataset[i][1])\n",
    "    return (np.stack(x0_list),np.stack(x1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = reshape_to_expected_input(dataset= train_set_data)\n",
    "del train_set_data\n",
    "valid_input = reshape_to_expected_input(dataset= valid_set_data)\n",
    "del valid_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 8\n",
    "num_workerssz = 5\n",
    "lr = 1e-4\n",
    "epochs = 120\n",
    "\n",
    "class ChalearnDataset(Dataset):\n",
    "    def __init__(self,imagedata,tagdata,transform=None):\n",
    "        self.imagedata=imagedata\n",
    "        self.tagdata=tagdata\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imagedata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_data=self.imagedata[idx]\n",
    "        image_data=torch.FloatTensor(image_data)\n",
    "        big_five_sorces=self.tagdata[idx]\n",
    "        big_five_sorces = torch.FloatTensor(big_five_sorces)\n",
    "        return image_data,big_five_sorces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChalearnDataset(imagedata=train_input[0],tagdata=train_input[1])\n",
    "valid_dataset = ChalearnDataset(imagedata=valid_input[0],tagdata=valid_input[1])\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss().to(device)  # 손실함수 MAE\n",
    "optimizer = torch.optim.AdamW(audio_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device(1):\n",
    "    \n",
    "    trainingEpoch_loss = []\n",
    "    validationEpoch_loss = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        val_avg_loss = 0\n",
    "        \n",
    "        audio_model.train()\n",
    "        for image_data, big_five_data in train_dataloader:\n",
    "            \n",
    "            image_data = image_data.to(device)\n",
    "            big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "            big_five_data = big_five_data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # 기울기가 0이 됩니다.\n",
    "            \n",
    "            hypothesis = audio_model(image_data)  # 모델의 예측 결과를 저장합니다.\n",
    "            \n",
    "            loss = criterion(hypothesis, big_five_data)  # 예측된 결과와 실제 태그 사이의 손실 값을 저장합니다.\n",
    "            \n",
    "            loss.backward()  # 역방향 전파입니다.\n",
    "            optimizer.step()  # 매개 변수를 업데이트합니다\n",
    "            \n",
    "            train_avg_loss += loss  # 훈련 손실의 평균치입니다\n",
    "        train_avg_loss=train_avg_loss/len(train_dataloader)\n",
    "        trainingEpoch_loss.append(train_avg_loss)\n",
    "        print('Epoch = {}, loss = {}'.format(i+1,train_avg_loss))\n",
    "        \n",
    "        with torch.no_grad():#validate\n",
    "            audio_model.eval()\n",
    "            for image_data, big_five_data in valid_dataloader:\n",
    "                \n",
    "                image_data = image_data.to(device)\n",
    "                \n",
    "                big_five_data = reduce(big_five_data,'b c d -> b c', 'max')\n",
    "                big_five_data = big_five_data.to(device)\n",
    "                \n",
    "                hypothesis = audio_model(image_data)\n",
    "                \n",
    "                val_loss = criterion(hypothesis, big_five_data)\n",
    "                val_avg_loss += val_loss\n",
    "                \n",
    "            val_avg_loss=val_avg_loss/len(valid_dataloader)\n",
    "            validationEpoch_loss.append(val_avg_loss)\n",
    "            print('Epoch = {}, val_loss = {}, 1 - MAE = {}'.format(i+1,val_avg_loss, 1 - val_avg_loss))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
